{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nix',\n",
       " 'core',\n",
       " 'boot',\n",
       " 'swapfile',\n",
       " 'initrd.img',\n",
       " 'home',\n",
       " 'cdrom',\n",
       " 'mnt',\n",
       " 'usr',\n",
       " 'media',\n",
       " 'dev',\n",
       " 'proc',\n",
       " '.VolumeIcon.icns',\n",
       " 'srv',\n",
       " 'run',\n",
       " 'vmlinuz',\n",
       " 'root',\n",
       " 'tmp',\n",
       " 'lib',\n",
       " 'bin',\n",
       " 'lost+found',\n",
       " 'snap',\n",
       " 'initrd.img.old',\n",
       " 'sys',\n",
       " 'var',\n",
       " '.VolumeIcon.png',\n",
       " 'sbin',\n",
       " 'etc',\n",
       " 'lib64',\n",
       " 'opt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('../..')\n",
    "sys.path.append('')\n",
    "    \n",
    "from random import randint, shuffle\n",
    "from random import random as rand\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import src.model.models as models\n",
    "from tqdm import tqdm\n",
    "import src.model.optim as optim\n",
    "import src.model.train as train\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.utils import set_seeds, get_device, truncate_tokens_pair, _sample_mask\n",
    "import src.data.tokenization as tokenization\n",
    "from src.data.data import Preprocess4Pretrain, SentPairDataset, seq_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file='./data/vocab.txt', do_lower_case=True)\n",
    "tokenize = lambda x: tokenizer.tokenize(tokenizer.convert_to_unicode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('the quick brown fox jumped over the lazy dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1996, 4248, 2829, 4419, 5598, 2058, 1996, 13971, 3899]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1996, 4248, 2829, 4419, 5598, 2058, 1996, 13971, 3899])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumped over the lazy dog'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_to_unicode('the quick brown fox jumped over the lazy dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A highlevel view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/273 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/273 [00:00<02:05,  2.17it/s]\u001b[A\n",
      "  3%|▎         | 7/273 [00:00<01:27,  3.04it/s]\u001b[A\n",
      "  3%|▎         | 9/273 [00:00<01:09,  3.78it/s]\u001b[A\n",
      "  6%|▌         | 16/273 [00:00<00:49,  5.23it/s]\u001b[A\n",
      "  7%|▋         | 19/273 [00:01<00:40,  6.23it/s]\u001b[A\n",
      "  8%|▊         | 22/273 [00:01<00:31,  7.89it/s]\u001b[A\n",
      " 10%|▉         | 27/273 [00:01<00:25,  9.80it/s]\u001b[A\n",
      " 11%|█         | 30/273 [00:01<00:19, 12.18it/s]\u001b[A\n",
      " 13%|█▎        | 35/273 [00:02<00:18, 12.98it/s]\u001b[A\n",
      " 15%|█▍        | 40/273 [00:02<00:14, 15.74it/s]\u001b[A\n",
      " 16%|█▌        | 43/273 [00:02<00:15, 14.82it/s]\u001b[A\n",
      " 18%|█▊        | 48/273 [00:02<00:12, 17.33it/s]\u001b[A\n",
      " 19%|█▊        | 51/273 [00:02<00:11, 19.20it/s]\u001b[A\n",
      " 20%|█▉        | 54/273 [00:02<00:13, 15.75it/s]\u001b[A\n",
      " 21%|██        | 56/273 [00:03<00:15, 14.35it/s]\u001b[A\n",
      " 22%|██▏       | 60/273 [00:03<00:12, 17.46it/s]\u001b[A\n",
      " 23%|██▎       | 64/273 [00:03<00:11, 18.21it/s]\u001b[A\n",
      " 25%|██▍       | 68/273 [00:03<00:10, 20.49it/s]\u001b[A\n",
      " 26%|██▌       | 71/273 [00:03<00:10, 19.82it/s]\u001b[A\n",
      " 28%|██▊       | 76/273 [00:04<00:10, 18.38it/s]\u001b[A\n",
      " 29%|██▉       | 79/273 [00:04<00:09, 19.90it/s]\u001b[A\n",
      " 31%|███       | 84/273 [00:04<00:09, 20.66it/s]\u001b[A\n",
      " 32%|███▏      | 87/273 [00:04<00:09, 19.24it/s]\u001b[A\n",
      " 33%|███▎      | 90/273 [00:04<00:08, 20.91it/s]\u001b[A\n",
      " 34%|███▍      | 93/273 [00:04<00:09, 19.36it/s]\u001b[A\n",
      " 35%|███▌      | 96/273 [00:05<00:10, 16.94it/s]\u001b[A\n",
      " 37%|███▋      | 100/273 [00:05<00:08, 19.36it/s]\u001b[A\n",
      " 38%|███▊      | 103/273 [00:05<00:08, 19.92it/s]\u001b[A\n",
      " 39%|███▉      | 106/273 [00:05<00:07, 20.91it/s]\u001b[A\n",
      " 40%|███▉      | 109/273 [00:05<00:08, 19.52it/s]\u001b[A\n",
      " 41%|████      | 112/273 [00:05<00:09, 16.10it/s]\u001b[A\n",
      " 42%|████▏     | 116/273 [00:06<00:08, 17.88it/s]\u001b[A\n",
      " 44%|████▍     | 120/273 [00:06<00:09, 16.89it/s]\u001b[A\n",
      " 45%|████▌     | 124/273 [00:06<00:07, 19.82it/s]\u001b[A\n",
      " 47%|████▋     | 128/273 [00:06<00:08, 17.09it/s]\u001b[A\n",
      " 48%|████▊     | 132/273 [00:07<00:07, 17.71it/s]\u001b[A\n",
      " 50%|████▉     | 136/273 [00:07<00:07, 19.57it/s]\u001b[A\n",
      " 51%|█████▏    | 140/273 [00:07<00:08, 15.66it/s]\u001b[A\n",
      " 54%|█████▍    | 148/273 [00:08<00:07, 15.89it/s]\u001b[A\n",
      " 57%|█████▋    | 156/273 [00:08<00:06, 17.28it/s]\u001b[A\n",
      " 60%|██████    | 164/273 [00:08<00:06, 17.45it/s]\u001b[A\n",
      " 63%|██████▎   | 172/273 [00:09<00:05, 17.27it/s]\u001b[A\n",
      " 66%|██████▌   | 180/273 [00:09<00:05, 18.03it/s]\u001b[A\n",
      " 69%|██████▉   | 188/273 [00:10<00:04, 18.94it/s]\u001b[A\n",
      " 72%|███████▏  | 196/273 [00:10<00:04, 16.87it/s]\u001b[A\n",
      " 75%|███████▍  | 204/273 [00:10<00:03, 19.75it/s]\u001b[A\n",
      " 78%|███████▊  | 212/273 [00:11<00:02, 21.70it/s]\u001b[A\n",
      " 79%|███████▉  | 217/273 [00:11<00:02, 24.79it/s]\u001b[A\n",
      " 81%|████████  | 220/273 [00:11<00:02, 19.39it/s]\u001b[A\n",
      " 84%|████████▎ | 228/273 [00:11<00:02, 20.02it/s]\u001b[A\n",
      " 85%|████████▌ | 233/273 [00:12<00:01, 24.06it/s]\u001b[A\n",
      " 86%|████████▋ | 236/273 [00:12<00:02, 16.45it/s]\u001b[A\n",
      " 89%|████████▉ | 244/273 [00:12<00:01, 16.65it/s]\u001b[A\n",
      " 92%|█████████▏| 252/273 [00:13<00:01, 18.29it/s]\u001b[A\n",
      " 95%|█████████▌| 260/273 [00:13<00:00, 20.14it/s]\u001b[A\n",
      " 98%|█████████▊| 268/273 [00:13<00:00, 24.66it/s]\u001b[A\n",
      "100%|██████████| 273/273 [00:13<00:00, 19.94it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file='./data/vocab.txt', do_lower_case=True)\n",
    "tokenize = lambda x: tokenizer.tokenize(tokenizer.convert_to_unicode(x))\n",
    "\n",
    "pipeline = [Preprocess4Pretrain(400,\n",
    "                                0.15,\n",
    "                                list(tokenizer.vocab.keys()),\n",
    "                                tokenizer.convert_tokens_to_ids,\n",
    "                                400,\n",
    "                                1,\n",
    "                                1,\n",
    "                                3)]\n",
    "data_iter = DataLoader(SentPairDataset('./data/wiki.test.tokens',\n",
    "                            16,\n",
    "                            tokenize,\n",
    "                            400,\n",
    "                            pipeline=pipeline), batch_size=16, collate_fn=seq_collate, num_workers=8)\n",
    "\n",
    "for batch in tqdm(data_iter):\n",
    "    input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next, original_ids = batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at it deeply\n",
    "We see that the text has lots of empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      " = Robert <unk> = \n",
      "\n",
      " \n",
      "\n",
      " Robert <unk> is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John <unk> in 2002 . In 2004 <unk> landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the <unk> <unk> Factory in London . He was directed by John <unk> and starred alongside Ben <unk> , Shane <unk> , Harry Kent , Fraser <unk> , Sophie Stanton and Dominic Hall . \n",
      "\n",
      " In 2006 , <unk> starred alongside <unk> in the play <unk> written by Mark <unk> . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by <unk> <unk> . How to Curse was performed at Bush Theatre in the London Borough of <unk> and Fulham . <unk> starred in two films in 2008 , <unk> <unk> by filmmaker Paris <unk> , and <unk> Punch directed by <unk> Blackburn . In May 2008 , <unk> made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series <unk> in November 2008 . He had a recurring role in ten episodes of the television series <unk> in 2010 , as \" <unk> Fletcher \" . <unk> starred in the 2011 film <unk> directed by Paris <unk> . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = './data/wiki.test.tokens'\n",
    "N = 5\n",
    "with open(filename) as myfile:\n",
    "    for i in range(N):\n",
    "        print(next(myfile))\n",
    "#     head = [next(myfile) for x in range(N)]\n",
    "# print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SentPairDataset`\n",
    "\n",
    "The key preprocessing object is the `SentPairDataset` object. As it pulls the data, it tokenizes and runs it through the pipeline.  \n",
    "An important note is the expected format of the input file. Each line should comprise of a single sentence (though not a deal breaker.) Importantly, each document needs to be line-seperated (`'\\n'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = SentPairDataset('./data/wiki.test.tokens',\n",
    "                        16,\n",
    "                        tokenize,\n",
    "                        400,\n",
    "                        pipeline=pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pull and tokenize the sentence pairs\n",
    "The `tokenize` function can take in any text. There is no need to do some special preprocessing to the original corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robert',\n",
       " 'bo',\n",
       " '##ult',\n",
       " '##er',\n",
       " 'is',\n",
       " 'an',\n",
       " 'english',\n",
       " 'film',\n",
       " ',',\n",
       " 'television']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_text = \"Robert Boulter is an English film, television and theatre actor. He had a guest-starring role on the television series The Bill in 2000. This was followed by a starring role in the play Herons written by Simon Stephens, which was performed in 2001 at the Royal Court Theatre.\"\n",
    "tokenize(orig_text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are pulling __pairs__ of segments (not sentences). Thus, a generated int, `len_tokens`, is a number less than half the `max_len`. 10% of the time, it will be even less. `len_tokens` is the length of the one of two segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% of the time, we get a random number of tokens \n",
    "# less than half the max_length\n",
    "len_tokens = randint(1, int(self.max_len / 2)) \\\n",
    "    if rand() < self.short_sampling_prob \\\n",
    "    else int(self.max_len / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When pulling the two segments, `read_tokens` tokenizes and returns each line.  \n",
    "Importantly, it handles the logic such that it _tries_ to achieve the expected length of half `max_len`.   \n",
    "  \n",
    "`discard_last_and_restart` is a vital argument for the first sentence. If the first segment already hits the end of the document, `discard_last_and_restart` will ditch the collected tokens and attempt to tokenize the first segment of the next token.\n",
    "  \n",
    "  A small note is that the sum of the two segments may well be larger than `max_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 / 0\n"
     ]
    }
   ],
   "source": [
    "# pulling the pairs of sentences\n",
    "tokens_a = self.read_tokens(self.f_pos, len_tokens, True)\n",
    "f_next = self.f_pos # `f_next` should be next point\n",
    "tokens_b = self.read_tokens(f_next, len_tokens, False)\n",
    "\n",
    "# SOP, sentence-order prediction\n",
    "is_next = rand() < 0.5 # whether token_b is next to token_a or not\n",
    "instance = (is_next, tokens_a, tokens_b) if is_next \\\n",
    "    else (is_next, tokens_b, tokens_a)\n",
    "print(f\"{len(tokens_a)} / {len(tokens_b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling possible exceptions:  \n",
    "1. End of document: When arriving at the end of the document, the second segment may end up being an empty list. Here, I implement an additional logic of splitting the first segment into two. \n",
    "2. End of file: it will reset the pointer of the file object, going back to the start of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no more tokens in the document\n",
    "# split between tokens_a and tokens_b, halfway\n",
    "if (len(tokens_b) == 0) & (len(tokens_a)>10):\n",
    "    half_split = int(len(tokens_a)/2)\n",
    "    tokens_b = tokens_a[half_split:]\n",
    "    tokens_a = tokens_a[:half_split]\n",
    "    \n",
    "    \n",
    "# there are no more tokens in the entire text file.\n",
    "if tokens_a is None or tokens_b is None: # end of file\n",
    "    self.f_pos.seek(0, 0) # reset file pointer\n",
    "\n",
    "    # re-read token\n",
    "    tokens_a = self.read_tokens(self.f_pos, len_tokens, True)\n",
    "    f_next = self.f_pos # `f_next` should be next point\n",
    "    tokens_b = self.read_tokens(f_next, len_tokens, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Preprocess4Pretrain` \n",
    "After pulling the data in the form of an `instance` which contains `is_next, tokens_a, tokens_b`, the next step is to preprocess the `instance` into the 8 `torch.tensor` objects that our transformer may ingest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = Preprocess4Pretrain(400,\n",
    "                            0.15,\n",
    "                            list(tokenizer.vocab.keys()),\n",
    "                            tokenizer.convert_tokens_to_ids,\n",
    "                            400,\n",
    "                            1,\n",
    "                            1,\n",
    "                            3)\n",
    "\n",
    "is_next, tokens_a, tokens_b = instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`truncate_tokens_pair` as it clearly states, will truncate the two segments to a size accommodating `max_len` and the special tokens.  \n",
    "`segment_ids` indicate which tokens belong to which segment.  \n",
    "`input_mask` indicates the useful tokens and the paddings.  \n",
    "Some reference for BERT: https://medium.com/@aieeshashafique/feature-extraction-from-bert-25887ed2152a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -3  for special tokens [CLS], [SEP], [SEP]\n",
    "truncate_tokens_pair(tokens_a, tokens_b, self.max_len - 3)\n",
    "\n",
    "# Add Special Tokens\n",
    "tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "segment_ids = [0]*(len(tokens_a)+2) + [1]*(len(tokens_b)+1)\n",
    "input_mask = [1]*len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sample the mask over which our generator will replace words.  \n",
    "`masked_pos` is just a positional index.  \n",
    "The naming is a bit strange. `masked_tokens` actually refer to the untouched tokens whereas `tokens` refer to the token sequence with the masked tokens.   \n",
    "`masked_weights` simply weights all of them equally. We assume that theyre all the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of prediction is sometimes less than max_pred when sequence is short\n",
    "n_pred = min(self.max_pred, max(1, int(round(len(tokens) * self.mask_prob))))\n",
    "\n",
    "original_ids = self.indexer(tokens)\n",
    "# For masked Language Models\n",
    "masked_tokens, masked_pos, tokens = _sample_mask(tokens, self.mask_alpha,\n",
    "                                                self.mask_beta, self.max_gram,\n",
    "                                                goal_num_predict=n_pred)\n",
    "# prev_masked_tokens = masked_tokens.copy()\n",
    "# print(len(masked_tokens))\n",
    "\n",
    "masked_weights = [1]*len(masked_tokens)\n",
    "\n",
    "# Token Indexing\n",
    "input_ids = self.indexer(tokens)\n",
    "masked_ids = self.indexer(masked_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we pad!  \n",
    "Originally, the author set them to be `max_pred` long but errors were encountered. It seems that the correct one should be in fact `max_len`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " ['\"',\n",
       "  'kiss',\n",
       "  'you',\n",
       "  '\"',\n",
       "  'was',\n",
       "  'well',\n",
       "  'received',\n",
       "  'by',\n",
       "  'contemporary',\n",
       "  'music',\n",
       "  'critics',\n",
       "  ',',\n",
       "  'who',\n",
       "  'centred',\n",
       "  'on',\n",
       "  'its',\n",
       "  'quality',\n",
       "  'of',\n",
       "  'production',\n",
       "  '.',\n",
       "  'both',\n",
       "  'rolling',\n",
       "  'stone',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'jon',\n",
       "  'do',\n",
       "  '##lan',\n",
       "  ',',\n",
       "  'who',\n",
       "  'praised',\n",
       "  'its',\n",
       "  'effectiveness',\n",
       "  ',',\n",
       "  'and',\n",
       "  'chris',\n",
       "  'payne',\n",
       "  'of',\n",
       "  'billboard',\n",
       "  ',',\n",
       "  'who',\n",
       "  'appreciated',\n",
       "  'the',\n",
       "  'melody',\n",
       "  ',',\n",
       "  'described',\n",
       "  '\"',\n",
       "  'kiss',\n",
       "  'you',\n",
       "  '\"',\n",
       "  'as',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'album',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'highlights',\n",
       "  '.',\n",
       "  'alexis',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'for',\n",
       "  'the',\n",
       "  'guardian',\n",
       "  'commended',\n",
       "  'the',\n",
       "  'track',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'chorus',\n",
       "  'as',\n",
       "  '\"',\n",
       "  'hard',\n",
       "  'to',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'from',\n",
       "  'your',\n",
       "  'brain',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'robert',\n",
       "  'cops',\n",
       "  '##ey',\n",
       "  'of',\n",
       "  'digital',\n",
       "  'spy',\n",
       "  'noted',\n",
       "  'the',\n",
       "  'song',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'possibility',\n",
       "  'to',\n",
       "  'become',\n",
       "  'an',\n",
       "  'international',\n",
       "  'hit',\n",
       "  ',',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'it',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  '.',\n",
       "  'a',\n",
       "  'reviewer',\n",
       "  'for',\n",
       "  'mtv',\n",
       "  'news',\n",
       "  'described',\n",
       "  'the',\n",
       "  'track',\n",
       "  \"'\",\n",
       "  's',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'as',\n",
       "  '\"',\n",
       "  'butterflies',\n",
       "  '@',\n",
       "  '-',\n",
       "  '@',\n",
       "  'inducing',\n",
       "  '\"',\n",
       "  ',',\n",
       "  'and',\n",
       "  'sam',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'of',\n",
       "  'idol',\n",
       "  '##ator',\n",
       "  'wrote',\n",
       "  'that',\n",
       "  '\"',\n",
       "  'kiss',\n",
       "  'you',\n",
       "  '\"',\n",
       "  'is',\n",
       "  'noticeably',\n",
       "  'a',\n",
       "  'stand',\n",
       "  '@',\n",
       "  '-',\n",
       "  '@',\n",
       "  'out',\n",
       "  'track',\n",
       "  'on',\n",
       "  'its',\n",
       "  'parent',\n",
       "  'album',\n",
       "  '.',\n",
       "  'melinda',\n",
       "  'newman',\n",
       "  ',',\n",
       "  'writing',\n",
       "  'for',\n",
       "  'hit',\n",
       "  '##fi',\n",
       "  '##x',\n",
       "  ',',\n",
       "  'regarded',\n",
       "  'the',\n",
       "  'song',\n",
       "  'as',\n",
       "  '\"',\n",
       "  'a',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  ',',\n",
       "  'electronic',\n",
       "  'infectious',\n",
       "  'di',\n",
       "  '##tty',\n",
       "  ',',\n",
       "  '\"',\n",
       "  'while',\n",
       "  'chris',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  ',',\n",
       "  'a',\n",
       "  'critic',\n",
       "  'from',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  ',',\n",
       "  'deemed',\n",
       "  'it',\n",
       "  'an',\n",
       "  '\"',\n",
       "  'amazing',\n",
       "  'pop',\n",
       "  'song',\n",
       "  '\"',\n",
       "  ',',\n",
       "  'lau',\n",
       "  '##ding',\n",
       "  'the',\n",
       "  'group',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'false',\n",
       "  '##tto',\n",
       "  'and',\n",
       "  'its',\n",
       "  '\"',\n",
       "  'head',\n",
       "  '@',\n",
       "  '-',\n",
       "  '@',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  '\"',\n",
       "  'chorus',\n",
       "  '.'],\n",
       " ['\"',\n",
       "  'kiss',\n",
       "  'you',\n",
       "  '\"',\n",
       "  'is',\n",
       "  'an',\n",
       "  'up',\n",
       "  '##tem',\n",
       "  '##po',\n",
       "  ',',\n",
       "  'upbeat',\n",
       "  'power',\n",
       "  'pop',\n",
       "  'song',\n",
       "  'which',\n",
       "  'runs',\n",
       "  'for',\n",
       "  'a',\n",
       "  'duration',\n",
       "  'of',\n",
       "  '3',\n",
       "  ':',\n",
       "  '04',\n",
       "  '(',\n",
       "  '3',\n",
       "  'minutes',\n",
       "  ',',\n",
       "  'four',\n",
       "  'seconds',\n",
       "  ')',\n",
       "  '.',\n",
       "  'the',\n",
       "  'track',\n",
       "  'features',\n",
       "  'electronic',\n",
       "  'effects',\n",
       "  ',',\n",
       "  'colossal',\n",
       "  'hooks',\n",
       "  ',',\n",
       "  'a',\n",
       "  '\"',\n",
       "  'na',\n",
       "  'na',\n",
       "  'na',\n",
       "  '\"',\n",
       "  'breakdown',\n",
       "  ',',\n",
       "  'and',\n",
       "  'a',\n",
       "  'motown',\n",
       "  '@',\n",
       "  '-',\n",
       "  '@',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'melody',\n",
       "  '.',\n",
       "  'one',\n",
       "  'direction',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'vocal',\n",
       "  'range',\n",
       "  'in',\n",
       "  'the',\n",
       "  'song',\n",
       "  'span',\n",
       "  'from',\n",
       "  'the',\n",
       "  'note',\n",
       "  'of',\n",
       "  'e',\n",
       "  '##4',\n",
       "  'to',\n",
       "  'c',\n",
       "  '♯',\n",
       "  '6',\n",
       "  '.',\n",
       "  'instrumentation',\n",
       "  'includes',\n",
       "  'guitar',\n",
       "  'strings',\n",
       "  ',',\n",
       "  'piano',\n",
       "  'lines',\n",
       "  'and',\n",
       "  'vocals',\n",
       "  '.',\n",
       "  'written',\n",
       "  'in',\n",
       "  'the',\n",
       "  'key',\n",
       "  'of',\n",
       "  'e',\n",
       "  'major',\n",
       "  ',',\n",
       "  'the',\n",
       "  'beat',\n",
       "  'is',\n",
       "  'set',\n",
       "  'in',\n",
       "  'common',\n",
       "  'time',\n",
       "  'and',\n",
       "  'moves',\n",
       "  'at',\n",
       "  'a',\n",
       "  'quick',\n",
       "  '90',\n",
       "  'beats',\n",
       "  'per',\n",
       "  'minute',\n",
       "  ',',\n",
       "  'according',\n",
       "  'to',\n",
       "  'the',\n",
       "  'digital',\n",
       "  'sheet',\n",
       "  'music',\n",
       "  'published',\n",
       "  'at',\n",
       "  'music',\n",
       "  '##notes',\n",
       "  '.',\n",
       "  'com',\n",
       "  'by',\n",
       "  'sony',\n",
       "  '/',\n",
       "  'atv',\n",
       "  'music',\n",
       "  'publishing',\n",
       "  '.',\n",
       "  'likewise',\n",
       "  ',',\n",
       "  'matt',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'from',\n",
       "  'allmusic',\n",
       "  'noted',\n",
       "  'that',\n",
       "  'the',\n",
       "  'track',\n",
       "  'is',\n",
       "  '\"',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'hyper',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'the',\n",
       "  'lyrical',\n",
       "  'content',\n",
       "  'regards',\n",
       "  'the',\n",
       "  'protagonist',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'in',\n",
       "  '##fat',\n",
       "  '##uation',\n",
       "  'with',\n",
       "  'a',\n",
       "  'significant',\n",
       "  'other',\n",
       "  ',',\n",
       "  'and',\n",
       "  'incorporates',\n",
       "  '<',\n",
       "  'un',\n",
       "  '##k',\n",
       "  '>',\n",
       "  'for',\n",
       "  'sexual',\n",
       "  'intercourse',\n",
       "  'in',\n",
       "  'the',\n",
       "  'lines',\n",
       "  '\"',\n",
       "  'if',\n",
       "  'you',\n",
       "  'don',\n",
       "  '’',\n",
       "  't',\n",
       "  'wanna',\n",
       "  'take',\n",
       "  'it',\n",
       "  'slow',\n",
       "  '/',\n",
       "  'and',\n",
       "  'you',\n",
       "  'just',\n",
       "  'wanna',\n",
       "  'take',\n",
       "  'me',\n",
       "  'home',\n",
       "  '/',\n",
       "  'baby',\n",
       "  'say',\n",
       "  'yeah',\n",
       "  ',',\n",
       "  'yeah',\n",
       "  ',',\n",
       "  'yeah',\n",
       "  ',',\n",
       "  'yeah',\n",
       "  ',',\n",
       "  'yeah',\n",
       "  '.',\n",
       "  '\"'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero Padding\n",
    "n_pad = self.max_len - len(input_ids)\n",
    "original_ids.extend([0]*n_pad)\n",
    "input_ids.extend([0]*n_pad)\n",
    "segment_ids.extend([0]*n_pad)\n",
    "input_mask.extend([0]*n_pad)\n",
    "\n",
    "# Zero Padding for masked target\n",
    "# originally the author constrained masked_ids, masked_pos and masked_weights to be length 75\n",
    "# but they should be of the same length\n",
    "# so I replace max_pred w max_len\n",
    "# prev_masked_id = masked_ids.copy()\n",
    "if self.max_len > len(masked_ids):\n",
    "    masked_ids.extend([0] * (self.max_len - len(masked_ids)))\n",
    "elif self.max_len > len(masked_ids):\n",
    "    raise ValueError(\"Strangely, the masked_ids is more than max_pred\")\n",
    "# assert self.max_pred == len(masked_ids), f\"self.max_pred {self.max_pred} vs shape of prev {len(prev_masked_id)} vs shape of new {len(masked_ids)}\"\n",
    "if self.max_len > len(masked_pos):\n",
    "    masked_pos.extend([0] * (self.max_len - len(masked_pos)))\n",
    "if self.max_len > len(masked_weights):\n",
    "    masked_weights.extend([0] * (self.max_len - len(masked_weights)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
