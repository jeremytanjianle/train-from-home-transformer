{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "# s = spm.SentencePieceProcessor(model_file='spm.model')\n",
    "# for n in range(5):\n",
    "#     s.encode('New York', out_type=str, enable_sampling=True, alpha=0.1, nbest=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.listdir('../../data/wiki.train.tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "params = ('--input=../../data/wiki.train.tokens --model_prefix=spm --vocab_size=5000')\n",
    "spm.SentencePieceTrainer.train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁New', '▁York']\n",
      "[195, 467]\n",
      "New York\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('spm.model')\n",
    "\n",
    "print( sp.EncodeAsPieces(input='New York') )\n",
    "print( sp.EncodeAsIds(input='New York') )\n",
    "print( sp.DecodeIds([195, 467]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodePieces(['▁New', '▁York'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId('▁New')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.IdToPiece('▁New')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge appeal of using SentencePiece is subword regularization. To enable this, we must use `sample_encode_as_pieces` or `SampleEncodeAsIds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'N', 'e', 'w', '▁York']\n",
      "['▁New', '▁York']\n",
      "['▁N', 'e', 'w', '▁York']\n",
      "['▁New', '▁', 'Y', 'or', 'k']\n",
      "['▁New', '▁York']\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "#     print( sp.SampleEncodeAsIds('New York', alpha=0.1, nbest_size=-1) )\n",
    "    print( sp.sample_encode_as_pieces('New York', alpha=0.1, nbest_size=-1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current FullTokenizer\n",
    "```\n",
    "class FullTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_tokens_to_ids(self.vocab, tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, tokens):\n",
    "        return convert_ids_to_tokens(self.vocab, tokens)\n",
    "\n",
    "    def convert_to_unicode(self, text):\n",
    "        return convert_to_unicode(text)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class tokenizer_abstract_Class(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def tokenize(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def convert_tokens_to_ids(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def convert_ids_to_tokens(self):\n",
    "        pass\n",
    "    \n",
    "#     def convert_to_unicode(self):\n",
    "#         return convert_to_unicode(text)\n",
    "    \n",
    "import sentencepiece as spm\n",
    "\n",
    "class SPTokenizer(tokenizer_abstract_Class):\n",
    "    def __init__(self, model_path='spm.model', nbest_size=-1, alpha=0.1):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(model_path)\n",
    "        self.nbest_size=nbest_size\n",
    "        self.alpha=alpha\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.sp.sample_encode_as_pieces(text, self.nbest_size, self.alpha)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.sp.PieceToId(token) for token in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.sp.IdToPiece(id_) for id_ in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized string: ['▁N', 'ew', '▁York']\n",
      "tokenized string id: [273, 2109, 467]\n",
      "recovered pieces: ['▁N', 'ew', '▁York']\n"
     ]
    }
   ],
   "source": [
    "string = \"New York\"\n",
    "\n",
    "tokenized_string = SPTokenizer('spm.model').tokenize(string) \n",
    "print(f\"tokenized string: {tokenized_string}\")\n",
    "\n",
    "tokenized_string_id = SPTokenizer('spm.model').convert_tokens_to_ids(tokenized_string)\n",
    "print(f\"tokenized string id: {tokenized_string_id}\")\n",
    "\n",
    "recovered_pieces =  SPTokenizer('spm.model').convert_ids_to_tokens(tokenized_string_id)\n",
    "print(f\"recovered pieces: {recovered_pieces}\")\n",
    "\n",
    "assert tokenized_string == recovered_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.sample_encode_as_pieces('the quick brown fox jumped over the lazy dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
